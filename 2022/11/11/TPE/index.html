<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hyperparameter Optimization TPE | Something about Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Machine learning models, especially complex neural networks, usually require a set of hyperparameters which can have a significant impact on the models. How to evaluate the models and select the best">
<meta property="og:type" content="article">
<meta property="og:title" content="Hyperparameter Optimization TPE">
<meta property="og:url" content="https://yarinamomo.github.io/2022/11/11/TPE/index.html">
<meta property="og:site_name" content="Something about Learning">
<meta property="og:description" content="Machine learning models, especially complex neural networks, usually require a set of hyperparameters which can have a significant impact on the models. How to evaluate the models and select the best">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/SMBO.png">
<meta property="og:image" content="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE_pxy.png">
<meta property="og:image" content="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE_formula_1.png">
<meta property="og:image" content="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE_formula_2.png">
<meta property="og:image" content="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE_formula_3.png">
<meta property="og:image" content="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE.png">
<meta property="article:published_time" content="2022-11-11T09:28:18.453Z">
<meta property="article:modified_time" content="2022-11-13T15:27:42.783Z">
<meta property="article:author" content="Yiran Wang">
<meta property="article:tag" content="Machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/SMBO.png">
  
    <link rel="alternate" href="/atom.xml" title="Something about Learning" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Something about Learning</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://yarinamomo.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-TPE" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/11/11/TPE/" class="article-date">
  <time class="dt-published" datetime="2022-11-11T09:28:18.453Z" itemprop="datePublished">2022-11-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Hyperparameter Optimization TPE
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Machine learning models, especially complex neural networks, usually require a set of hyperparameters which can have a significant impact on the models. How to evaluate the models and select the best set of hyperparameters is vital for the model to achieve a desired outcome. There are automatic hyperparameter tuning algorithms. For example, grid search evaluates every combination of the hyperparameter candidates and return the most optimal set, but the exhaustive search often takes a long time than one can accept. Random search can be done in a fixed number of evaluations, but it simply tries on different settings randomly and it would be of luck to get a good result. By tuning hyperparameters manually, one can take the previous evaluations into account when selecting the next hyperparameter combinations. However, the process can be tedious. Here we introduce a method that can smartly select the next combination of hyperparameters to try, based on the historical evaluations.</p>
<span id="more"></span>

<h3 id="1-Bayesian-Optimization"><a href="#1-Bayesian-Optimization" class="headerlink" title="1. Bayesian Optimization"></a>1. Bayesian Optimization</h3><p>Bayesian optimization is a global optimization with a sequential strategy. Sequential means to running evaluations one after another. It is usually used for models where the objective function is too computational complex or with an unknown form of structure. </p>
<p>The main idea of Bayesian Optimization is to not deal with the detail of the objective function, but to treat it as a random function with a prior. The <strong>prior</strong> p(y) reflects the assumption of the objective function <em>f</em> and it will be updated by evaluating on <strong>data</strong> p(x|y) to form the <strong>posterior</strong> p(y|x). Based on the posterior distribution, an gain criteria (i.e., Expected Improvement, <strong>EI</strong> or Expected Losses) will be constructed to select the next query point (combination of hyperparameters). The next query point should be selected by following the <strong>exploration and exploitation</strong> tradeoff where the model can achieve a relatively high performance or low loss with relatively few queries.</p>
<h3 id="2-SMBO"><a href="#2-SMBO" class="headerlink" title="2. SMBO"></a>2. SMBO</h3><p>Sequential model based optimization (SMBO) is a clear formalization of Bayesian Optimization. It takes the previous evaluations into account to select the next combination of hyperparameters. When selecting future set of hyperparameter, it tries to search on ranges that may potentially give a better outcome based on Bayesian reasoning without calculating gradient. It can handle real-valued, discrete as well as conditional types of hyperparameters. Additionally, it is also capable of dealing with large number of hyperparameters. The following shows the outline of SMBO:</p>
<p align="center"> <img width="100%" src="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/SMBO.png"> </p>

<!-- regular html comment
# ```
# SMBO(f,M_0,n,S)
#   H <- {}   # evaluation history
#   for i <- 1 to n:  # number of evaluation trials
#     x* <- argmin_x S(x,M_i-1)  # get the next combination of hyperparameters to evaluate
#     calculate f(x*)  # evaluate the model with the new x*
#     H <- {H, (x*,f(x*))}  # save the history
#     M_t updated with H  # M is updated to be a better approximation of f
# ```
-->

<p>where:</p>
<ul>
<li>X: a domain of hyperparameters, search space</li>
<li>f: Objective function, takes a set of hyperparameters and output a score that needs to minimize or maximize</li>
<li>M: Surrogate function, try to approximate f</li>
<li>S: Selection function, to select the next set of hyperparameters to evaluate</li>
<li>H: History, (hyperparameters, score) pairs of the past evaluation trials</li>
</ul>
<p>The <strong>surrogate function</strong> or <strong>acquisition function</strong> does the tradeoff between exploring new areas in the search space and exploiting the areas that are evaluated and potentially have optimal candidates. There are different algorithms to optimize the Expected Improvement (<strong>EI</strong>), among which, <strong>Tree-based Paren Estimater (TPE)</strong> is well-known by its efficiency and promising outcome.</p>
<h3 id="2-TPE"><a href="#2-TPE" class="headerlink" title="2. TPE"></a>2. TPE</h3><p>As a kernel estimator, TPE is designed to construct in tree structures. It models the <strong>posterior</strong> distribution p(y|x) (build a surrogate function <em>M</em> to approximate objective function <em>f</em>) by modeling the <strong>likelihood</strong> p(x|y) and <strong>prior</strong> p(y). </p>
<p>To model p(x|y), the TPE transforms the configuration space <em>X</em> based on types: uniform to truncated Gaussian mixture, log-uniform to exponential truncated Gaussian mixture, categorical to re-weighted categorical. The distributions of the configuration prior is replaced by non-parametric densities. These transformations, along with different hyperparameter candidates <em>x</em> in the non-parametric densities, represent a learning algorithm to produce many densities over <em>X</em>. Here are the two densities used in TPE:</p>
<p align="center"> <img width="100%" src="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE_pxy.png"> </p>

<!-- regular html comment
# ```
# p(x|y) = l(x) if y < y*  #l(x) is density formed using hyperparameter candidates that make loss of f less than y*
# p(x|y) = g(x) if y >= y*  #g(x) is density formed using the rest hyperparameter candidates
# ```
-->

<p>The TPE chooses y* to be some quantile r of the observed y values so that </p>
<p align="center"> <img width="15%" src="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE_formula_1.png"> </p>

<!-- regular html comment
$$p(y < y^*) = \gamma$$
-->

<p>The Expected Improvement is:</p>
<p align="center"> <img width="80%" src="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE_formula_2.png"> </p>

<!-- regular html comment
$$EI_{y^*}(x)=\int_{-\infty}^{y^*}(y^*-y)p(y|x)dy = \int_{-\infty}^{y^*}(y^*-y) \frac{p(x|y)p(y)}{p(x)} dy$$ 
$$\int_{-\infty}^{y^*}(y^*-y) p(x|y)p(y) dy = l(x)\int_{-\infty}^{y^*} (y^*-y) p(y)dy = \gamma {y^*} l(x) - l(x) \int_{-\infty}^{y^*}p(y)dy$$
$$p(x) = \int p(x|y)p(y)dy = \gamma l(x) + (1-\gamma) g(x)$$
-->

<p>So,</p>
<p align="center"> <img width="60%" src="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE_formula_3.png"> </p>

<!-- regular html comment
$$EI_{y^*}(x)=\frac{\gamma {y^*} l(x) - l(x) \int_{-\infty}^{y^*}p(y)dy}{\gamma l(x) + (1-\gamma) g(x)} \propto (\gamma + \frac{g(x)}{l(x)}(1-\gamma))^{-1}$$
-->

<p>The continuous types of hyperparameter configurations <em>X &#x3D; {x1, x2, …, xk}</em> are specified by a prior (uniform or Gaussian or log-uniform distribution) over some interval. The TPE then transform the prior to an equally weighted Gaussian mixtures (for uniform prior) centered at each <em>xi</em>. The standard deviation of each Gaussian is set to the greater of the distances to the left and right neighbor (including the upper and lower limit of the uniform range) with clipping to limit within a reasonable range. For discrete hyperparameters, the prior would be a vector where each element is assigned to a probability pi, and in the posterior vector, the elements are proportional to <em>N*pi+Ci</em>  where <em>N</em> is the total number of elements and <em>C</em> is the count of the occurrences of the <em>i</em>th choice. To maximize <strong>EI</strong>, the hyperparameter candidates with the high probability under <em>l(x)</em> and low probability under <em>g(x)</em> are preferred. The tree structured form of <em>l</em> and <em>g</em> makes it easy for evaluation and selection of the optimal x* with the highest <em>EI</em>.</p>
<p>Overall, the algorithm of TPE can be summarized as below:</p>
<p align="center"> <img width="100%" src="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/TPE/TPE.png"> </p>

<!-- regular html comment
# ```
# X # define the hyperparameter configuration, search space
# f # define the objective function which takes in a set of hyperparameters and output a score to minimize/maximize
# TPE(f,X,n)
#   H <- {}   # evaluation history
#   for i <- 1 to j: # for j random selected sets of hyperparameters
#     x' <- random_sample(X) # random select a set of hyperparameters
#     f(x') # calculate objective score
#     H <- {H, (x',f(x'))}  # save the history
#   for i <- 1 to n:  # number of evaluation trials
#     sort the selected x based on their scores
#     divide into two groups x1 (best score) and x2 (rest) based on some quatile r
#     model l(x1) and g(x2) by Parzen Estimators (density mixtures)
#     x* <- argmin_{sample(l(x1))} l(x1)/g(x2)  # draw samples x* from l(x1) and get sets of x that yield the minimum/maxmum of l(x1)/g(x2)
#     calculate f(x*)  # evaluate the model with the new x*
#     H <- {(x*,f(x*))}  # update the selected x and their scores
# ```
-->

<h3 id="3-TPE-Hyperopt"><a href="#3-TPE-Hyperopt" class="headerlink" title="3. TPE - Hyperopt"></a>3. TPE - Hyperopt</h3><p>Hyperopt is a Python library to select hyperparameters automatically based on SMBO. It starts with defining two essential parts of the process:</p>
<ul>
<li>Hyperparameter configuration, search space</li>
<li>Objective function</li>
</ul>
<p>The following is a simple example about how to use the package, more detailed information can be found <a target="_blank" rel="noopener" href="https://github.com/hyperopt/hyperopt/wiki/FMin">here</a>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 3.1 import library</span><br><span class="line">from hyperopt import hp, fmin, tpe, STATUS_OK, Trials</span><br><span class="line"></span><br><span class="line"># 3.2 define search space for hyperparameters</span><br><span class="line">space =&#123;&quot;lr&quot;: hp.loguniform(&quot;m_lr&quot;, np.log(1e-4), np.log(1e-2)),</span><br><span class="line">        &quot;activation&quot;: hp.choice(&quot;m_activation&quot;, (&#x27;relu&#x27;, &#x27;tanh&#x27;, &#x27;linear&#x27;, &#x27;sigmoid&#x27;)),</span><br><span class="line">        &quot;dropout&quot;: hp.uniform(&quot;m_do&quot;, 0, 0.7),</span><br><span class="line">        &quot;hidden_size&quot;: hp.choice(&quot;m_hiddensize&quot;, ((64,32,16), (32,16,8), (64,64)))&#125;</span><br><span class="line">        </span><br><span class="line"># 3.3 define objective function</span><br><span class="line">def objective():</span><br><span class="line">  model_mlp.initialize(space) # self-defined configurable model</span><br><span class="line">  history = model_mlp.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val))</span><br><span class="line">  min_loss = min(history.history[&#x27;val_loss&#x27;]) # if want to minimize the validation loss</span><br><span class="line">  return &#123;&#x27;loss&#x27;: min_loss, &#x27;status&#x27;: STATUS_OK&#125;</span><br><span class="line">  </span><br><span class="line"># 3.4 save the results</span><br><span class="line">trials = Trials() # stores the return values during the evaluation procedure, for plotting and analyzing</span><br><span class="line"></span><br><span class="line"># 3.5 evaluation</span><br><span class="line">best = fmin(fn=objective,</span><br><span class="line">            space=space,</span><br><span class="line">            algo=tpe.suggest,</span><br><span class="line">            max_evals=200,</span><br><span class="line">            trials=trials)</span><br><span class="line">best # the selected set of hyperparameters which gives the best result of objective function</span><br></pre></td></tr></table></figure>

<p>Note that TPE requires many iterations to converge to an optimal solution, and it is recommended to run it for at least 200 iterations. Every iteration needs to evaluate a full generated model (with a predefined epoch times on the full training&#x2F;validation datasets). Thus it is likely that this process may take from a few hours up to a few days to complete, especially for deep neural networks or models with a large dataset.</p>
<h4 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h4><p><a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf">Algorithms for Hyper-Parameter Optimization</a></p>
<p><a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf">Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://yarinamomo.github.io/2022/11/11/TPE/" data-id="clafc50l70000ej5f07jb7chz" data-title="Hyperparameter Optimization TPE" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning/" rel="tag">Machine learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/11/20/CPP/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          C++ - Pointer, Reference, Virtual Methods and Struct
        
      </div>
    </a>
  
  
    <a href="/2022/10/18/DCA/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Explanation on DCA (Deep Count Autoencoder)</div>
    </a>
  
</nav>

  
</article>




  <div id="disqus_thread"></div>
  <script>
      /**
      *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
      *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
      var disqus_config = function () {
        this.page.url = 'https://yarinamomo.github.io/2022/11/11/TPE/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = '1668158898453'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
      };
      (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://https-yarinamomo-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">ABOUT</h3>
    <div class="widget">
      <div class="row">
        <img src="https://github.com/yarinamomo/yarinamomo.github.io/raw/main/images/profile.jpg" alt="profile picture" width=80% >
      </div>
      Email: <b>yarinawangmomo@gmail.com</b>
      LinkedIn：<a target="_blank" rel="noopener" href="https://www.linkedin.com/in/yiran-wang-a6201a9a/">Yiran Wang</a>
    </div>
  </div>


  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bioinformatics/" rel="tag">Bioinformatics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Vision/" rel="tag">Computer Vision</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Configuration/" rel="tag">Configuration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-learning/" rel="tag">Machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Programming/" rel="tag">Programming</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Bioinformatics/" style="font-size: 10px;">Bioinformatics</a> <a href="/tags/Computer-Vision/" style="font-size: 10px;">Computer Vision</a> <a href="/tags/Configuration/" style="font-size: 15px;">Configuration</a> <a href="/tags/Machine-learning/" style="font-size: 20px;">Machine learning</a> <a href="/tags/Programming/" style="font-size: 15px;">Programming</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/11/22/CSharp/">Interesting things in C#</a>
          </li>
        
          <li>
            <a href="/2022/11/20/CPP/">C++ - Pointer, Reference, Virtual Methods and Struct</a>
          </li>
        
          <li>
            <a href="/2022/11/11/TPE/">Hyperparameter Optimization TPE</a>
          </li>
        
          <li>
            <a href="/2022/10/18/DCA/">Explanation on DCA (Deep Count Autoencoder)</a>
          </li>
        
          <li>
            <a href="/2022/10/12/GPUCPU/">Is GPU much faster than CPU in machine learning?</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 Yiran Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>